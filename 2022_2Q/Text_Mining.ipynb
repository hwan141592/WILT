{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1daf5c",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa602d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')        # once is enough\n",
    "nltk.download('stopwords')    # once is enough\n",
    "# stopwords.words('english')[0:10]\n",
    "\n",
    "text = 'Statistics skills, and programming skills are equally important for analytics. Statistics skills, and domain knowledge are important for analytics. I like reading books and travelling.'\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f18ce4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a69e84",
   "metadata": {},
   "source": [
    "#### chunck report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3680a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* original text:\t I go to school\n",
      "* tokenized words:\t ['I', 'go', 'to', 'school']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'I go to school'\n",
    "words = nltk.word_tokenize(text)\n",
    "print('* original text:\\t', text)\n",
    "print('* tokenized words:\\t', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3223b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* original text:\t I go to school\n",
      "* tokenized words:\t ['I', 'go', 'to', 'school']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'I go to school'\n",
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c015d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, lang='english'):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    punt_removed= [w for w in words if w.lower() not in string.punctuation]\n",
    "    text = \" \".join(punt_removed)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords= stopwords.words(lang)\n",
    "    stopwords_removed= [w for w in words if w.lower() not in lang_stopwords]\n",
    "    text = \" \".join(stopwords_removed)\n",
    "    return \" \".join(text.split())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a29c1",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text_processed)\n",
    "TEXT_WORD_LIST = tokens\n",
    "TEXT_STRING = ' '.join(tokens)\n",
    "display(TEXT_WORD_LIST[0:5], TEXT_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ef795",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = TEXT_STRING\n",
    "\n",
    "# x, y = np.ogrid[:300, :300]\n",
    "\n",
    "# mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "# mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(background_color=\"white\") #, repeat=True) #, mask=mask)\n",
    "wc.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf0e54",
   "metadata": {},
   "source": [
    "# Wordcloud.2\n",
    " - with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import multidict as multidict\n",
    "import numpy as np\n",
    "import random\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def getFrequencyDictForText(sentence):\n",
    "    fullTermsDict = multidict.MultiDict()\n",
    "    tmpDict = {}\n",
    "\n",
    "    # making dict for counting frequencies\n",
    "    for text in sentence.split(\" \"):\n",
    "        if re.match(\"a|the|an|the|to|in|for|of|or|by|with|is|on|that|be\", text):\n",
    "            continue\n",
    "        val = tmpDict.get(text, 0)\n",
    "        tmpDict[text.lower()] = val + 1\n",
    "    for key in tmpDict:\n",
    "        fullTermsDict.add(key, tmpDict[key])\n",
    "    return fullTermsDict\n",
    "\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "\n",
    "def makeImage(text):\n",
    "    alice_mask = np.array(Image.open(\"./img/bg.png\"))\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000, mask=alice_mask)\n",
    "    wc.generate_from_frequencies(text)   # generate word cloud\n",
    "    plt.figure(figsize=(12,4))       \n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.imshow(wc.recolor(random_state=505), interpolation=\"bilinear\")   # color_func=grey_color_func, \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "# d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "# text = open(path.join(d, 'alice.txt'), encoding='utf-8')\n",
    "# text = text.read()\n",
    "\n",
    "wc.to_file(\"./img/wc_img.png\")\n",
    "makeImage(getFrequencyDictForText(TEXT_STRING))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26ab5a",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e725b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print('bag of words vector :', vector.fit_transform(corpus).toarray()) \n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print('vocabulary :',vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 지정 불용어 제외\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CounterVectorizer()가 제공하는 불용어 제외\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eca602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK에서 지원하는 불용어 제외\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fbe46",
   "metadata": {},
   "source": [
    "# Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5df9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f26ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56045dd9",
   "metadata": {},
   "source": [
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9adf4e",
   "metadata": {},
   "source": [
    " - https://www.tutorialspoint.com/python_text_processing/python_remove_stopwords.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d5672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
