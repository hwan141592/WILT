[Q.] TF-IDF 란?


* 카운트 기반의 텍스트 표현 방법 메인 2가지
 - 1) DTM: Document Term Matrix
 - 2) TF-IDF: Term Frequency - Inverse Document Frequency
 - 이 2가지 방법을 통해서 '텍스트'를 '수치화'할 수 있음. >> 이후에 통계기법 적용 등

* 단어의 표현 방법 2가지
 - 1) 국소 표현 [Local Representation]
     >> 단어 그 자체만 보고 특정 값을 맵핑하여 단어를 표현하는 방법
     >> [이산 표현: Discrete Representation]
 - 2) 분산 표현 [Distributed Representation]
     >> 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법
     >> [연속 표현: Continuous Representation]

* 챕터 2개
 - 챕터.1) Bag of Words: 국소 표현에 대해서...
 - 챕터.2) 워드임베딩: 연속 표현에 대해서...

* Bag of words
 - 단어의 등장 순서를 고려하지 않음
 - 빈도수 기반의 단어 표현 방법

* Bag of words란?
 - 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법. 
 - e.g.) sentence = '나의 이름과 너의 이름은 다르다'
         vocab = {'나':0, '의':1, '이름':2, '과':3, '너':4, '은':5, '다르다':6}
         bag_of_words_vector >> [1, 2, 2, 1, 1, 1, 1]
         >> '의'(인덱스.1), '이름'(인덱스.2) 은 각각 2번씩 나오므로.

* Bag of words 의 쓰임새
 - 각 단어가 등장한 횟수를 수치화하므로 어떤 문서에서 어떤 단어가 많이 등장했는지를 파악 가능
 - 즉, 어떤 단어가 많이 등장했는지를 바탕으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰임
 - e.g.) 문서간 유사도를 구하는 문제 
         (방정식, 미분, 적분 등의 단어가 자주 나오면 수학 관련 문서로 분류하는 등)

* python
 - CounterVectorizer()
 - from sklearn.feature_extraction.text import CountVectorizer

* 불용어를 제거한 BoW 만들기(python)
 - from sklearn.feature_extraction.text import CountVectorizer
 - from nltk.corpus import stopwords

* [DTM] 문서-단어 행렬
 - [DTM] Document-Term Matrix
 - 서로 다른 문서들의 BoW들을 결합한 표현 방법 >> 서로 다른 문서들을 비교 가능
 - 행과 열을 반대로 선택하여 TDM 이라고도 한다.

* DTM의 한계
 - sparse representation(희소 표현)
 - one-hot vector: 단어 집합의 크기가 벡터의 차원이 되고 대부분의 값이 0이 되는 벡터
 - one-hot 벡터는 공간적 낭비와 컴퓨팅 리소스를 많이 소모
 - DTM도 마찬가지로 각 행을 문서벡터라고 했을 때, 
   각 문서벡터의 차원은 one-hot벡터와 마찬가지로 전체 단어 집합의 크기를 가짐.
 - one-hot벡터, DTM과 같이 대부분의 값이 0인 표현을 
   sparse vector(희소벡터) 또는 sparse matrix(희소행렬)라고 함.
 - 희소벡터는 많은 양의 저장공간과 높은 계산복잡도를 요구하므로 전처리가 중요.
 - 전처리를 통해 단어 집합의 크기를 줄이는 일은 BoW 표현을 사용하는 모델에서 중요.
 - 단어 집합의 크기를 줄이는 방법
    >> 1) 구두점, 빈도수가 낮은 단어, 불용어를 제거
    >> 2) 어간이나 표제어 추출을 통해 단어를 정규화

* 질문해볼 점
 - 각 문서에 포함되어 있는 중요한 단어와 불필요한 단어들은 어떻게 구분할까?
 - DTM에 불용어와 중요한 단어에 대해서 가중치를 줄 수 있는 방법은 없을까?
 - 해답으로서 'TF-IDF'가 나옴.

* TF-IDF (가중치)
 - Term Frequency - Inverse Document Frequency
 - 각 DTM 안에 있는 각 단어에 대한 중요도를 계산할 수 있음.
 - 즉, 기존의 DTM을 사용하는 것보다 더 많은 정보를 고려하여 문서들을 비교할 수 있음.
 - 많은 경우, DTM보다 더 좋은 성능을 낸다.

* TF-IDF
 - DTM 안의 각 단어들마다 중요한 정도를 가중치로 주는 방법
 - 단어의 빈도와 역문서 빈도(문서의 빈도에 특정 식을 취함 - '다른 문서와 비교했을 때')를 이용
 - 만드는 방식: DTM을 만든 후 TF-IDF 가중치를 부여

* TF-IDF의 활용
 - 문서의 유사도를 구하는 작업
 - 검색 시스템에서 검색 결과의 중요도를 정하는 작업
 - 문서 내에서 특정 단어의 중요도를 구하는 작업 등

* TF-IDF 계산
 - TF-IDF는 TF와 IDF를 곱한 값을 의미
 - TF, DF, IDF 3가지 요소를 이해해야 한다.
   (1) TF: 특정 문서 d에서의 특정 단어 t의 등장 횟수 >> 문서 내에 얼마나 자주 등장하는가?
   (2) DF: 특정 단어 t가 등장한 문서의 수
   (3) IDF: DF에 반비례하는 수 (역수, 로그화 포함)

* TF-IDF의 이해/해석
 - 즉, TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단.
 - 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단.
 - TF-IDF 값이 낮으면 중요도가 낮은 단어고 TF-IDF 값이 높으면 중요도가 높은 단어로 판단.
 - 'the'나 'a'와 같은 불용어는 모든 문서에 자주 등장하므로 
   자연스럽게 불용어의 TF-IDF 값이 낮아지게 되는 것.
 - "TF-IDF 점수가 높은 단어일수록 다른 문서에는 많지 않고 해당 문서에서 자주 등장하는 단어이다"


[Reference] 
 - https://wikidocs.net/22650
 - https://nesoy.github.io/articles/2017-11/tf-idf
