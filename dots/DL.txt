[Q.] Deep Learning 이란 무엇인가요?

Deep Learning?
 - input set과 output set을 mapping하는 것을 학습(learn)하는 것
 - neural network에서 hidden layer를 많이 사용한 것을 'deep'한 network라고 함
 - deep한 network의 application을 'deep learning'이라고 함
 - 즉, deep learning이란 "hidden layer를 많이 사용한 neural network의 application"

Feedforward Neural Networks
 - a.k.a. 'Multilayer Perceptron(MLP)'
 - (keywords) Input Layer, Hidden Layer, Output Layer
 - (keywords) intercept, features, Error back propagation

Neural Network 3가지
 - ANN: Artificial Neural Network
 - RNN: Recurrent Neural Network
 - CNN: Convolutional Neural Network

------------------------------------------

ANN
 - Artificial Neural Network

NN
 - Neural Network
 - 연결된 layer들의 시리즈
 - one end: 관측치의 feature 
 - the other end: target value (e.g. class label)

Feedforward Neural Network
 - a.k.a. 'Multilayer Perceptron(MLP)'
 - [GOAL] output layer 값들이 target 값들과 일치시키는 것
 - (keywords) Forward-Propagation, Back-Propagation
 - (keywords) Update weights
 - (keywords) output과 target 비교

Feedforward NN 만들기(construction)
 1) hidden layer와 output layer의 모든 layer들마다
    각 layer에 추가할 unit의 갯수와 activation function을 정의한다.
 2) network에서 사용할 hidden layer의 갯수를 정의한다.
  - layer 갯수가 많을수록 network가 더 복잡한 relationship을 학습
  - but, layer 갯수가 많을수록 동시에 computational cost가 커짐
 3) (필요시) output layer의 activation function 구조를 정의한다.
 4) loss function을 정의한다.
  - loss function: 얼마나 예측값이 실제값과 match하는지를 측정하는 함수
 5) optimizer를 정의한다.
  - 'walking around' the loss function
 6) performance를 evaluate할 metric을 하나 이상 정의/선택한다.
  - e.g.) accuracy

Feedforward NN의 구성요소 (details)
 - [A] Hidden Layers
 - [B] Output Layer
 - [C] Loss Function
 - [D] Optimizer

[A] Hidden Layers의 각 Unit
 - 1) 많은 input을 받는다.
 - 2) parameter 값에 따라서 각각의 input에 weight를 준다.
 - 3) weighted input 값들을 모두 합한다. (bias 포함)
 - 4) activate function을 적용한다.
 - 5) 자신(unit)의 output의 다음 layer의 unit에게 전달한다.

[B] Output Layer Pattern
 - 1) Binary Classification
   >> unit 하나 & sigmoid activation function
 - 2) Multi-class Classification
   >> k unit & softmax activation function
   >> k: target 클래스의 갯수
   >> softmax: (0,1) 구간의 확률분포로 normalize 해주는 역할
 - 3) Regression
   >> unit 하나 & no activation function

[C] Loss Function
 - 1) Binary Classification
   >> target class가 2개인 classification problem
   >> Loss Function: 'Cross-Entropy'
   >> also a.k.a Logarithmic loss
 - 2) Multiclass Classification
   >> target class가 2개 보다 많은 classification problem
   >> Loss Function: 'Categorical Cross-Entropy'
 - 3) Regression
   >> real-value quantity를 예측(predict)하는 problem
   >> Loss Function: Mean Squared Error(MSE)

[D] Optimizers
 - Stochastic gradient descent
 - Stochastic gradient descent with momentum 
 - Root mean square propagation
 - Adaptive moment estimation

------------------------------------------

RNN
 - Recurrent Neural Network

Problem
 - 매 point마다 이전 단어에 기반해서 다음 단어를 예측하는 문제

RNN 특징
 - Feedforward NN과 비슷
 - Feedforward NN과 다른 점: RNN에는 feedback loop가 있다
 - feedback loop가 있다는 것의 의미
   >> 이전(previous) step들의 특징을 현재(current) step에 반영!
 - 이러한 feedback loop를 가지는 RNN의 architecture는 시퀀스(순서형 정보)를 생성함
 - 상황을 시뮬레이션하고 종합적인 데이터를 생성하는 시퀀스를 생성
 - sequence data 모델링에 이상적임
 - sequence data 예시) speech text mining, image captioning, time series prediction, robot control, langauge modeling 등
 - [RNN의 핵심] 
   >> Output layer -> Hidden layer로 피드백
   >> Hidden layer -> 이전 Hidden layer로 피드백
   >> Hidden layer -> Input layer로 피드백

RNN의 단점
 - ... 

LSTM
 - Long Short Term Memory (LSTM)
 - ...

RNN vs. LSTM
 - ... 


















